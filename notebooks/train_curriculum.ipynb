{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curriculum Learning - Late Game First\n",
    "\n",
    "Train agent on partial board states from known solutions for April 14.\n",
    "Start with 2 pieces remaining (easiest), progress to 7 pieces remaining (hardest).\n",
    "\n",
    "Episode structure: agent sees board with N-M pieces placed, chooses 1 action, gets reward based on whether it's in valid solution set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from torch.distributions import Distribution\n",
    "\n",
    "from apad_puzzle_rl.training_utils import (\n",
    "    CurriculumProgressionCallback,\n",
    "    GradNormCallback,\n",
    "    TimerCallback,\n",
    "    make_curriculum_env,\n",
    ")\n",
    "\n",
    "# Disable validation to avoid Simplex constraint issues\n",
    "Distribution.set_default_validate_args(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./curriculum_ppo_logs_1/PPO_14\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 1015     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1536     |\n",
      "| train/             |          |\n",
      "|    grad_norm       | 0        |\n",
      "---------------------------------\n",
      "Step 3000, 3s elapsed, 2256s remaining\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 890        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09031832 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.34      |\n",
      "|    explained_variance   | -1.3       |\n",
      "|    grad_norm            | 0.253      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0467    |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.116     |\n",
      "|    value_loss           | 0.274      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 868        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 4608       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.23908101 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.15      |\n",
      "|    explained_variance   | 0.0673     |\n",
      "|    grad_norm            | 0.187      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0868    |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.135     |\n",
      "|    value_loss           | 0.165      |\n",
      "----------------------------------------\n",
      "\n",
      "ðŸŽ“ Curriculum advanced: 2 â†’ 3 pieces remaining (success rate: 85.0%)\n",
      "Step 6000, 7s elapsed, 2341s remaining\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.48       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 856        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17447422 |\n",
      "|    clip_fraction        | 0.534      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.96      |\n",
      "|    explained_variance   | 0.0209     |\n",
      "|    grad_norm            | 0.206      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0793    |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.105     |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1         |\n",
      "|    ep_rew_mean          | 0.65      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 842       |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 7680      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2320771 |\n",
      "|    clip_fraction        | 0.552     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.19     |\n",
      "|    explained_variance   | 0.171     |\n",
      "|    grad_norm            | 0.313     |\n",
      "|    learning_rate        | 0.003     |\n",
      "|    loss                 | -0.0881   |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.111    |\n",
      "|    value_loss           | 0.118     |\n",
      "---------------------------------------\n",
      "Step 9000, 11s elapsed, 2406s remaining\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.66       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 832        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29972348 |\n",
      "|    clip_fraction        | 0.698      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.65      |\n",
      "|    explained_variance   | -0.0329    |\n",
      "|    grad_norm            | 0.279      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0716    |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.126     |\n",
      "|    value_loss           | 0.176      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.8        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 826        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 10752      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.25823504 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | 0.0999     |\n",
      "|    grad_norm            | 0.268      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0899    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.117     |\n",
      "|    value_loss           | 0.149      |\n",
      "----------------------------------------\n",
      "Step 12000, 15s elapsed, 2434s remaining\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 821        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 14         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21489187 |\n",
      "|    clip_fraction        | 0.571      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.21      |\n",
      "|    explained_variance   | 0.102      |\n",
      "|    grad_norm            | 0.268      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0823    |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.101     |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 816        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 13824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16917826 |\n",
      "|    clip_fraction        | 0.491      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.0356     |\n",
      "|    grad_norm            | 0.326      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0681    |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0931    |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n",
      "\n",
      "ðŸŽ“ Curriculum advanced: 3 â†’ 4 pieces remaining (success rate: 85.0%)\n",
      "Step 15000, 19s elapsed, 2464s remaining\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1         |\n",
      "|    ep_rew_mean          | 0.67      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 809       |\n",
      "|    iterations           | 10        |\n",
      "|    time_elapsed         | 18        |\n",
      "|    total_timesteps      | 15360     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1533043 |\n",
      "|    clip_fraction        | 0.44      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.928    |\n",
      "|    explained_variance   | 0.134     |\n",
      "|    grad_norm            | 0.194     |\n",
      "|    learning_rate        | 0.003     |\n",
      "|    loss                 | -0.0742   |\n",
      "|    n_updates            | 90        |\n",
      "|    policy_gradient_loss | -0.0807   |\n",
      "|    value_loss           | 0.0976    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.75       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 794        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 16896      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18271697 |\n",
      "|    clip_fraction        | 0.52       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.11      |\n",
      "|    explained_variance   | 0.0634     |\n",
      "|    grad_norm            | 0.209      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0819    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0975    |\n",
      "|    value_loss           | 0.127      |\n",
      "----------------------------------------\n",
      "Step 18000, 23s elapsed, 2526s remaining\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.76       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 788        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 23         |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18260753 |\n",
      "|    clip_fraction        | 0.488      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | 0.0525     |\n",
      "|    grad_norm            | 0.29       |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0757    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0916    |\n",
      "|    value_loss           | 0.133      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.87       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 781        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 19968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19070582 |\n",
      "|    clip_fraction        | 0.462      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.926     |\n",
      "|    explained_variance   | 0.132      |\n",
      "|    grad_norm            | 0.211      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0623    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0873    |\n",
      "|    value_loss           | 0.116      |\n",
      "----------------------------------------\n",
      "Step 21000, 27s elapsed, 2558s remaining\n",
      "\n",
      "ðŸŽ“ Curriculum advanced: 4 â†’ 5 pieces remaining (success rate: 85.0%)\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.75       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 27         |\n",
      "|    total_timesteps      | 21504      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14310093 |\n",
      "|    clip_fraction        | 0.353      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.761     |\n",
      "|    explained_variance   | 0.0193     |\n",
      "|    grad_norm            | 0.36       |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0434    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.067     |\n",
      "|    value_loss           | 0.1        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 770        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 23040      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16705936 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.721     |\n",
      "|    explained_variance   | 0.0812     |\n",
      "|    grad_norm            | 0.321      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0485    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0705    |\n",
      "|    value_loss           | 0.0974     |\n",
      "----------------------------------------\n",
      "Step 24000, 32s elapsed, 2596s remaining\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 764        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 32         |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16221994 |\n",
      "|    clip_fraction        | 0.392      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.801     |\n",
      "|    explained_variance   | -0.0237    |\n",
      "|    grad_norm            | 0.371      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0544    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0746    |\n",
      "|    value_loss           | 0.13       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 758        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 34         |\n",
      "|    total_timesteps      | 26112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14616695 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.697     |\n",
      "|    explained_variance   | 0.0546     |\n",
      "|    grad_norm            | 0.296      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0434    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0682    |\n",
      "|    value_loss           | 0.102      |\n",
      "----------------------------------------\n",
      "Step 27000, 36s elapsed, 2629s remaining\n",
      "\n",
      "ðŸŽ“ Curriculum advanced: 5 â†’ 6 pieces remaining (success rate: 85.0%)\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 752        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 36         |\n",
      "|    total_timesteps      | 27648      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14207524 |\n",
      "|    clip_fraction        | 0.318      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.625     |\n",
      "|    explained_variance   | 0.126      |\n",
      "|    grad_norm            | 0.23       |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0518    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0671    |\n",
      "|    value_loss           | 0.102      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1         |\n",
      "|    ep_rew_mean          | 0.79      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 744       |\n",
      "|    iterations           | 19        |\n",
      "|    time_elapsed         | 39        |\n",
      "|    total_timesteps      | 29184     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1490612 |\n",
      "|    clip_fraction        | 0.314     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.621    |\n",
      "|    explained_variance   | 0.12      |\n",
      "|    grad_norm            | 0.415     |\n",
      "|    learning_rate        | 0.003     |\n",
      "|    loss                 | -0.0427   |\n",
      "|    n_updates            | 180       |\n",
      "|    policy_gradient_loss | -0.0591   |\n",
      "|    value_loss           | 0.1       |\n",
      "---------------------------------------\n",
      "Step 30000, 41s elapsed, 2680s remaining\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 736        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 41         |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16785641 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.582     |\n",
      "|    explained_variance   | 0.2        |\n",
      "|    grad_norm            | 0.334      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0132    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0549    |\n",
      "|    value_loss           | 0.109      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 729        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 44         |\n",
      "|    total_timesteps      | 32256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13032375 |\n",
      "|    clip_fraction        | 0.262      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.5       |\n",
      "|    explained_variance   | 0.317      |\n",
      "|    grad_norm            | 0.284      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0405    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0529    |\n",
      "|    value_loss           | 0.081      |\n",
      "----------------------------------------\n",
      "Step 33000, 46s elapsed, 2732s remaining\n",
      "\n",
      "ðŸŽ“ Curriculum advanced: 6 â†’ 7 pieces remaining (success rate: 85.0%)\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 721        |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 46         |\n",
      "|    total_timesteps      | 33792      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15973108 |\n",
      "|    clip_fraction        | 0.249      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.479     |\n",
      "|    explained_variance   | 0.233      |\n",
      "|    grad_norm            | 0.317      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0356    |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0415    |\n",
      "|    value_loss           | 0.0837     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 712        |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 49         |\n",
      "|    total_timesteps      | 35328      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09983602 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.466     |\n",
      "|    explained_variance   | 0.284      |\n",
      "|    grad_norm            | 0.418      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | -0.0132    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    value_loss           | 0.0833     |\n",
      "----------------------------------------\n",
      "Step 36000, 51s elapsed, 2789s remaining\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 704        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 52         |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16722155 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.408     |\n",
      "|    explained_variance   | 0.337      |\n",
      "|    grad_norm            | 0.5        |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | 0.0386     |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m MaskablePPO(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-3\u001b[39m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Train with automatic curriculum progression\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2_000_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTimerCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mGradNormCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCurriculumProgressionCallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# level_thresholds={2: 0.9, 3: 0.97, 4: 0.98, 5: 0.97, 6: 0.85, 7: 0.80}\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurriculum_model_v6\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/rl/lib/python3.10/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:454\u001b[0m, in \u001b[0;36mMaskablePPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, use_masking, progress_bar)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 454\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_masking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/rl/lib/python3.10/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:233\u001b[0m, in \u001b[0;36mMaskablePPO.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps, use_masking)\u001b[0m\n\u001b[1;32m    230\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(obs_tensor, action_masks\u001b[38;5;241m=\u001b[39maction_masks)\n\u001b[1;32m    232\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 233\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:137\u001b[0m, in \u001b[0;36mSubprocVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m--> 137\u001b[0m     results \u001b[38;5;241m=\u001b[39m [remote\u001b[38;5;241m.\u001b[39mrecv() \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes]\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     obs, rews, dones, infos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:137\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m--> 137\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\u001b[43mremote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes]\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     obs, rews, dones, infos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/rl/lib/python3.10/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/rl/lib/python3.10/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/rl/lib/python3.10/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create vectorized environments - start at easiest level (2 pieces remaining)\n",
    "n_envs = 6\n",
    "starting_level = 2  # pieces remaining\n",
    "\n",
    "env = SubprocVecEnv(\n",
    "    [lambda: make_curriculum_env(pieces_remaining=starting_level) for _ in range(n_envs)]\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = MaskablePPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    tensorboard_log=\"./curriculum_ppo_logs_1/\",\n",
    "    verbose=1,\n",
    "    batch_size=256,\n",
    "    n_steps=256,\n",
    "    ent_coef=0.0001,\n",
    "    learning_rate=3e-3,\n",
    ")\n",
    "\n",
    "# Train with automatic curriculum progression\n",
    "model.learn(\n",
    "    total_timesteps=2_000_000,\n",
    "    callback=[\n",
    "        TimerCallback(),\n",
    "        GradNormCallback(),\n",
    "        CurriculumProgressionCallback(\n",
    "            env,\n",
    "            min_episodes=100,\n",
    "            verbose=1,  # level_thresholds={2: 0.9, 3: 0.97, 4: 0.98, 5: 0.97, 6: 0.85, 7: 0.80}\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model.save(\"curriculum_model_v6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Level: 2 pieces remaining\n",
      "  Agent success:  89.0%\n",
      "  Random baseline: 54.0%\n",
      "  Advantage:      +35.0%\n",
      "\n",
      "Level: 3 pieces remaining\n",
      "  Agent success:  85.0%\n",
      "  Random baseline: 34.0%\n",
      "  Advantage:      +51.0%\n",
      "\n",
      "Level: 4 pieces remaining\n",
      "  Agent success:  87.0%\n",
      "  Random baseline: 11.0%\n",
      "  Advantage:      +76.0%\n",
      "\n",
      "Level: 5 pieces remaining\n",
      "  Agent success:  88.0%\n",
      "  Random baseline: 3.0%\n",
      "  Advantage:      +85.0%\n",
      "\n",
      "Level: 6 pieces remaining\n",
      "  Agent success:  71.0%\n",
      "  Random baseline: 1.0%\n",
      "  Advantage:      +70.0%\n",
      "\n",
      "Level: 7 pieces remaining\n",
      "  Agent success:  86.0%\n",
      "  Random baseline: 2.0%\n",
      "  Advantage:      +84.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from apad_puzzle_rl.envs.curriculum_env import CurriculumAPADEnv\n",
    "\n",
    "\n",
    "def evaluate_curriculum_agent(model, pieces_remaining=2, n_episodes=100):\n",
    "    \"\"\"Evaluate agent at specific curriculum level.\n",
    "\n",
    "    Returns:\n",
    "        dict with success_rate, random_baseline, advantage\n",
    "    \"\"\"\n",
    "    agent_success = 0\n",
    "    random_success = 0\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        # Agent episode\n",
    "        test_env = CurriculumAPADEnv(4, 14, pieces_remaining=pieces_remaining)\n",
    "        obs, info = test_env.reset()\n",
    "\n",
    "        action_masks = info[\"action_mask\"]\n",
    "        action, _ = model.predict(obs, action_masks=action_masks, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "\n",
    "        agent_success += reward > 0.5\n",
    "\n",
    "        # Random baseline\n",
    "        random_env = CurriculumAPADEnv(4, 14, pieces_remaining=pieces_remaining)\n",
    "        obs, info = random_env.reset()\n",
    "\n",
    "        valid_actions = np.where(info[\"action_mask\"])[0]\n",
    "        if len(valid_actions) > 0:\n",
    "            action = np.random.choice(valid_actions)\n",
    "            obs, reward, terminated, truncated, info = random_env.step(action)\n",
    "            random_success += reward > 0.5\n",
    "\n",
    "    agent_rate = agent_success / n_episodes\n",
    "    random_rate = random_success / n_episodes\n",
    "\n",
    "    return {\n",
    "        \"success_rate\": agent_rate,\n",
    "        \"random_baseline\": random_rate,\n",
    "        \"advantage\": agent_rate - random_rate,\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate at each curriculum level\n",
    "for pieces_remaining in range(2, 8):\n",
    "    results = evaluate_curriculum_agent(model, pieces_remaining=pieces_remaining)\n",
    "    print(f\"\\nLevel: {pieces_remaining} pieces remaining\")\n",
    "    print(f\"  Agent success:  {results['success_rate']:.1%}\")\n",
    "    print(f\"  Random baseline: {results['random_baseline']:.1%}\")\n",
    "    print(f\"  Advantage:      {results['advantage']:+.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Game Tests\n",
    "\n",
    "Test curriculum-trained model on full 8-piece games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 4.004\n",
      "1000 4.004\n",
      "1500 4.008\n",
      "2000 4.5\n",
      "2500 4.813\n",
      "3000 4.501\n",
      "3500 4.517\n",
      "4000 4.766\n",
      "4500 4.013\n",
      "5000 4.474\n",
      "5500 4.5\n",
      "6000 4.25\n",
      "6500 4.57\n",
      "7000 4.067\n",
      "7500 4.063\n",
      "8000 4.798\n",
      "8500 4.075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     22\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, action_masks\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     25\u001b[0m     step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Code/personal/apad-puzzle-rl/apad_puzzle_rl/envs/apad_env.py:289\u001b[0m, in \u001b[0;36mAPADEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# Cache masks after state change\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_action_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_action_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m n_remaining_pieces \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremaining_pieces)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_remaining_pieces \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandicap:  \u001b[38;5;66;03m# win\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/personal/apad-puzzle-rl/apad_puzzle_rl/envs/apad_env.py:329\u001b[0m, in \u001b[0;36mAPADEnv.action_masks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m available_pieces \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremaining_pieces)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m piece_id \u001b[38;5;129;01min\u001b[39;00m available_pieces:\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chirality, rotation, position \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_valid_placements(piece_id):\n\u001b[1;32m    330\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_action(piece_id, chirality, rotation, position)\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_islands:\n",
      "File \u001b[0;32m~/Code/personal/apad-puzzle-rl/apad_puzzle_rl/envs/apad_env.py:446\u001b[0m, in \u001b[0;36mAPADEnv._iter_valid_placements\u001b[0;34m(self, piece_id)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m position \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m43\u001b[39m):\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_valid_placement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpiece_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchirality\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m (chirality, rotation, position)\n",
      "File \u001b[0;32m~/Code/personal/apad-puzzle-rl/apad_puzzle_rl/envs/apad_env.py:246\u001b[0m, in \u001b[0;36mAPADEnv._is_valid_placement\u001b[0;34m(self, piece_id, chirality, rotation, position)\u001b[0m\n\u001b[1;32m    244\u001b[0m r, c \u001b[38;5;241m=\u001b[39m row \u001b[38;5;241m+\u001b[39m dr, col \u001b[38;5;241m+\u001b[39m dc\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Bounds check\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m r \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size \u001b[38;5;129;01mor\u001b[39;00m c \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m c \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Occupancy check\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from apad_puzzle_rl.envs.apad_env import APADEnv\n",
    "\n",
    "mean_step_count = 0\n",
    "\n",
    "# Play until we get a win on April 14\n",
    "env = APADEnv(4, 14)\n",
    "obs, info = env.reset()\n",
    "step_count = 0\n",
    "i = 0\n",
    "interval = 500.0\n",
    "\n",
    "while step_count < 8:\n",
    "    i += 1\n",
    "    if i % interval == 0:\n",
    "        print(i, round(mean_step_count, 3))\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=False, action_masks=info[\"action_mask\"])\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        step_count += 1\n",
    "        if done:\n",
    "            mean_step_count = (mean_step_count + step_count) / 2.0\n",
    "\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "print(f\"Found win after {i} attempts\")\n",
    "env.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 14 (trained):\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "Win rate: 0.0%\n",
      "\n",
      "April 15 (unseen):\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "Win rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "from apad_puzzle_rl.envs.apad_env import APADEnv\n",
    "\n",
    "\n",
    "def evaluate_win_rate(model, month, day, n_episodes=100):\n",
    "    \"\"\"Evaluate model win rate on complete games.\"\"\"\n",
    "    env = APADEnv(month, day)\n",
    "    wins = 0\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=False, action_masks=info[\"action_mask\"])\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "        wins += terminated\n",
    "\n",
    "    return wins / n_episodes\n",
    "\n",
    "\n",
    "# Test on April 14 (trained date)\n",
    "print(\"April 14 (trained):\")\n",
    "win_rate_414 = evaluate_win_rate(model, 4, 14, n_episodes=100)\n",
    "print(f\"Win rate: {win_rate_414:.1%}\\n\")\n",
    "\n",
    "# Test on April 15 (generalization)\n",
    "print(\"April 15 (unseen):\")\n",
    "win_rate_415 = evaluate_win_rate(model, 4, 15, n_episodes=100)\n",
    "print(f\"Win rate: {win_rate_415:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
